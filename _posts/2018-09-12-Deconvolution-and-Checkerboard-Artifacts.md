---
title: 反卷积与棋盘格效应分析
description: 
categories:
 - 翻译
tags: [棋盘格效应， 反卷积]
---
### 简介
当我们仔细的看通过神经网络生成的图片时，我们经常会看到一种棋盘格样式的瑕疵。在一些场景下会更常见，但是最近很多模型都有类似的问题。这种棋盘格瑕疵在强烈色彩的图片中会尤为凸显。这些瑕疵产生的原因实际上十分简单，避免他们的方法也是如此。

### 反卷积与重叠
当我们用神经网络生成图片时，我们会从低分辨率和高层次的描述中构建它们。这使得网络可以表达粗略的图片并向其中填充细节。

为了做到这一点，我们需要构想一些可以从低分辨率到高分辨率的方法。我们通常可以用反卷积操作来实现。一般来说，反卷积层允许模型用小图中的每一个点来「绘制」更大的一块正方形。

（反卷积有很多解释和其他的别名，包括转置卷积）。在此文中，为了简化我们统一用「反卷积」一词。

不幸的是，反卷积很容易有不均匀的重叠，使得某些地方比其他地方有更多的隐喻色彩。特别是，当核尺寸不能被步长整除的时候，反卷积会有不均匀的重叠。虽然理论上神经网络可以仔细的学习权重参数来避免产生这种瑕疵，但是事实上神经网络不能完全避免这种情况的发生。

这种重叠的样式体现在两个维度上。两个坐标轴上不均匀的重叠相乘，得到了类似棋盘格一样不同大小的特性。

事实上，不均匀的重叠在二维的情况下会更严重。因为当两种模式相乘，不均匀的模式会被平方。比如，在一维的情况下，一个步长为2，尺寸为3的反卷积输出数量为输入的两倍，但是在二维情况下，系数就变为了4。

如今，神经网络通常用在生成图像的时候通常用多层反卷积，通过不断的迭代从低像素图片构建高像素图片。尽管这些堆叠的反卷积可以在一定程度上消除棋盘效应，但他们经常复合，在不同尺度上产生这种瑕疵。

我们经常在成功模型的最后一层见到步长为1的反卷积，他们确实在减弱瑕疵方面十分有效。它们可以消除频率整除其大小的棋盘格效应，也可以减少其他频率小于尺寸的瑕疵。但是，这种效应仍然在最近的模型中出现。

除了我们之前观察到的高频棋盘格效应，早期的反卷积可以产生低频的瑕疵，我们会在后文详细介绍。

倘若输出非常规的颜色，这些瑕疵会变的尤为凸显。由于神经网络层可以学习到偏置，输出平均的颜色是相对容易的。一种颜色——比如亮红——越是偏离平均颜色，反卷积需要做出越多的贡献。

### 重叠与学习
实际上，不均匀重叠，尽管是一个很有效的框架，但是其本身有点过于简单了。无论是好是坏，我们的模型会为他们的反卷积学习权重参数。

理论上，我们的模型可以仔细地写入不均匀重叠的位置，使得实处能被均匀的平衡。

这种平衡的操作的实现是很棘手的，特别是当多个通道相互影响的时候。避免棋盘格瑕疵会很大程度上限制可能的过滤器，牺牲模型的容量。实际上，神经网络很难完全避免这些问题。

事实上，不仅是不均匀重叠的模型不能避免这种问题，均匀重叠的模型也会学习产生同样的瑕疵的卷积核。尽管不是像不均匀重叠的默认行为，对于均匀重叠的反卷积仍然是很容易产生瑕疵的。

完全避免这种棋盘格效应对于过滤器来说仍然是一个很严重的限制，实际上这些瑕疵仍然出现在这些模型中，虽然他们看起来确实更轻微了。

这里可能有很多因素在起作用。比如说，在生成对抗网络中，一个问题可能是判别器和它的梯度。但是这个问题的一大部分看起来是反卷积。再好的情况下，反卷积是脆弱的，因为即使很小心的调整尺寸的大小，它依然很容易表达瑕疵。在最坏的情况下，生成瑕疵是反卷积的默认行为。

有没有对于棋盘格效应更有抵抗能力的上采样方法呢？

### 更好的上采样
为了避免棋盘格效应，我们想要一种除了反卷积之外的其他上采样方式。不像反卷积，这种上采样方式不会将产生瑕疵作为其默认行为。理想情况下，它会走的更深，对于瑕疵有抵抗能力。

一种方式是确保你使用的卷积核尺寸可以被步长所整除，来避免重叠的问题。这和「亚像素卷积」类似，一种最近在超分辨率中取得成功的上采样方法。然而，尽管这种方法起到了作用，这荏苒很容易使得反卷积产生棋盘格效应。

另一个方法是将上采样到更高的分辨率和利用卷积计算特征分离开来。比如，你可以先调整图片的尺寸（利用近邻差值和双线性差值），然后再做卷积。这看起来是一个自然的方法，类似的方法在超分辨率中也起到了好的作用。

反卷积和不同的缩放卷积方式都是线性操作，可以用矩阵去解释。上图对于人情它们之间的区别很有帮助。反卷积对于每一个输出窗口有一个唯一
实体，缩放卷积会用集中权重的方式来抑制高频瑕疵。

### 代码
缩放卷积在tensorflow中可以简单的恶用tf.image.resize_images()来实现。对于最好的结果，是在tf.nn.con2d()卷积之前使用tf.pad()来避免边缘的瑕疵。

### 图像生成结果
我们的实验结果表明，最近邻缩放后接卷积在很多问题上都表现的很好。

我们发现这在生成对抗网络中起到了帮助。简单的将标准的反卷积层替换成缩放卷积能使得不同频率的瑕疵消失。

事实上，在展开训练之前，不同的瑕疵是可见的。如果我们观察随机权重初始化的生成器产生的图片，我们已经可以看到以下瑕疵。

这表明这些瑕疵的产生是因为生成图片的方法，而不是对抗训练的结果。（这也表明我们可以在不得到缓慢的训练反馈之前就判断其是否是一个好的生成器设计）

另外一个这些瑕疵不是生成对抗网络特有的原因是我们也在其他模型中见到了它们，同时也发现用缩放卷积来上采样可以有效的抑制这种棋盘格效应。比如说，在实时艺术风格迁移一文中，神经网络用来直接生成风格迁移的图片。我们发现它们很容易受的棋盘格效应的影响（尤其当损失函数没有显性的去抑制瑕疵的时候）。然而，将反卷积层替换为缩放卷积层之后，这些瑕疵也消失了。

### 梯度中的瑕疵
每当我们计算卷积层的梯度，我们就要在反馈通道中做一次反卷积。这会在梯度中造成棋盘格效应，就像我们用反卷积来生成图片一样。

在特征可视化领域中，图像模型梯度中的噪声是一个巨大的挑战。从某种程度上说，特征可视化的方法必须要对这些噪声做出补偿。

比如，DeepDream似乎会在不同的瑕疵之间造成破坏性的干扰，比如说同时优化多个特征或者在多个偏移（offsets）和尺度（scales）上进行优化的时候。特别是，在不同偏移中优化时产生的「抖动」（jitter）也能抵消一些棋盘格的效应。

尽管一些瑕疵是标准的棋盘格效应，另一些则是无组织的高频效应。我们认为这些是由最大池化造成的。最大池化之前被认为与此高频瑕疵有关。

最近的特征可视化工作已经意识到并且为高频梯度组件作出了补偿。一个问题是，是否存在更好的神经网络结构使得这些工作变得「非必要」。

这些梯度的瑕疵是否影响了生成对抗网络？如果梯度瑕疵可以影响一张被神经网络梯度优化的图片，我们也认为它可以影响一系列由生成器产生的图片，因为它们是被生成对抗网络中的判别器优化过的。

我们也发现这种情况也存在于其他一些情况。当生成器既不偏向也不对抗棋盘格效应的时候，判别器中的大步长卷积会产生这种效应。

目前还不是很清楚这些梯度瑕疵会带来什么更宽泛的影响。一个思路是一些神经元会在相邻的神经元中得到多次任意的梯度。同样的，神经网络会更偏向与输入图片中的一些像素，具体情况还不是很清楚。

一些像素点会比其他像素点更多的影响神经网络的输出这一事实可能会夸大对抗性反样本的作用。因为导数只专注于少量的像素点，这些像素点中的小的扰动可能会对输出有大的影响。

### 结论
用反卷积来产生图片的标准方法尽管是成功的，但仍然存在理论上的一些问题，导致在生成的图片中产生棋盘格效应。用一些其他自然的方法去代替能避免这种瑕疵。

这对于我们来说是一个激动人心的机会。它表明可以在神经网络架构中通过谨慎地思考，寻找一种低门槛的方法，这种方法能让我们找到清晰的解决问题的方法。

我们提供了一个易用的解决方案，提高了用神经网络生成图像的质量。我们期待看到这种方法会被怎样使用，以及它是否能对音频之类的领域有所帮助，这类领域中的高频伪影是尤其棘手的问题。

来源：https://distill.pub/2016/deconv-checkerboard/